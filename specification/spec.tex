\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\setlength{\parindent}{15pt}
\author{Nicholas Clarke}
\title{Mercury specification}

\newcounter{paracounter}
\newcommand{\npar}{\par\noindent\refstepcounter{paracounter}\theparacounter.\space}
\begin{document}
\maketitle
\begin{abstract}
We present a specification for Mercury, a hypothetical pipeline system designed for use in human genetics workflows. Drawn from previous experience managing and running genetics pipelines, we identify a number of specific capabilities that would be desirable for the purpose of provenance tracking, repeatability and optimality of experiment workflows.
\end{abstract}
\tableofcontents
\section{Introduction}
\npar Within human genetics, there is a need to be able to design and execute specific workflows (pipelines) for genome analysis on a reliable, repeatable and efficient basis. At present, HGI runs a variety of pipelines for human genetics projects. These are run against a Linux cluster using Lustre-backed storage by the following two tools: 
\begin{itemize}
\item \href{https://github.com/VertebrateResequencing/vr-pipe}{VRPipe} runs as the \textit{orchestration} framework for current pipelines. It takes a description of the workflow (as a Perl script) and is responsible for tracking dependencies between jobs, submitting them to the executor and monitoring their progress.
\item \href{http://www-03.ibm.com/systems/technicalcomputing/platformcomputing/products/lsf/}{LSF} runs as the executor, responsible for taking an individual job and executing it on some compute resource. LSF additionally has the ability to manage job dependencies, but these features are not currently in use at the Sanger.
\end{itemize}
\npar Whilst these tools work suitably well for the general usage pattern at the Sanger, they show a number of limitations which it would be desirable to work around. In particular, HGI is interested in the areas of \textbf{data provenance}, \textbf{experiment repeatability}, \textbf{experiment isolation}, \textbf{pipeline optimisation} and \textbf{pipeline security}, hereafter referred to as PRIOS.
\npar Human Genetics has recently taken delivery of a new cluster (hgs4) intended primarily for pipeline usage. This presents us with the opportunity to re-evaluate our current toolset and practices and think about what we would really like in a pipeline execution system. This document presents a write-up of a series of meetings held on the topic, and is intended to serve as a specification for a potential future system embodying these desires.
\npar This document should ideally present the requirements for such a future system, and serve equally well as a guideline for the design of a new system or as a standard to evaluate existing systems by. Inevitably, however, we shall fall into the trap of starting to design a new system rather than just specify it. Where possible, sections that veer into design or even implementation will be flagged as such; it is quite likely, however, that some will escape. The reader is asked to bear this in mind when applying this document.
\section{Goals}
\npar In the introduction, we introduced some high-level goals in which HGI has a particular interest. These are not requirements in their own right, being too high-level, but should give rise to requirements in the same way as other top-level goals such as 'the system must be usable'. In this section, we'll take a look at these goals, elaborating on and discussing them in order to provide context and motivation for the requirements introduced in the following sections.
\subsection{Data Provenance}
\label{sec:provenance}
\npar Data provenance is fundamentally about being able to ask the question ``Where did this data come from?'' For a given set of data, we want to be able to identify which pipeline(s) it was created from, what steps were involved, where the initial data came from, how the jobs were distributed amongst machines and what was running on the machines at the time? As far as possible, we would like to be able to identify precisely what happened to cause the data to be generated as they were.
\npar There are various justifications for this. Partially, it is about repeatability (as in the next section) - being able to identify how something came about makes it much easier to repeat it. Partially it is about data integrity - being certain that results were generated using the precise input data which you thought they were. Optimisation also calls for provenance data - once we understand how precisely data were generated, we know how expensive they are to recreate and how important it is to store them. And, in part, we want to be able to document the process for users' own purposes, so they can check what happened as part of an analysis run by HGI.
\npar Implementing provenance tracking leads to a number of requirements. We need to keep track of pipelines both as designed (in general, without specific reference to data) and as executed (on which hosts, at what time). We need to ensure that external dependencies are explicitly captured (which in turn leads to the isolation goal), and isolated both in the planning stage (separating external I/O from internal - e.g. passing data between steps in a pipeline) and in the execution stage, preventing steps from unknowingly accessing external resources.
\subsection{Experiment Repeatability}
\label{sec:repeatability}
\npar Following on from provenance, we have the goal of making our pipelines repeatable. This goes a step further than provenance; rather than simply identifying where data comes from, we would like to be able to recreate it, in spite of potential changes to the underlying infrastructure. 
\npar This gives rise to the ideas of encapsulation already implemented in Mercury capsules, wrapping up an environment suitable to run a step of the pipeline. Whilst capsules provide repeatability of individual steps, we are also forced to consider repeatability of the entire pipeline, which requires us to suitably abstract away from specific hardware or infrastructure constraints. This is one advantage to moving away from the current LSF-based approach, for example.
\npar Also covered under repeatability is the requirement to rerun an experiment whilst changing a fixed number of factors, and tracking effects. For example, we might want to rerun an old pipeline using a new version of a particular piece of software to see whether it meaningfully alters the output. This same idea applies to rerunning a pipeline on new data.
\subsection{Experiment Isolation}
\npar Isolation is, in many ways, an enabling goal. Isolation of experiment processing allows us to be certain of the provenance of generated results, and enables us to move the isolated experiment to another system to repeat it. By isolating each pipeline (and each step of the pipeline), we enable meaningful statistics to be gathered about resource usage and running profile to allow for optimisation, and prevent other systems from interfering with the processing in order to provide security.
\npar Isolation principally sets requirements on the execution of individual jobs, where we need to isolate the actual processing from other jobs on the same host, as well as from the wider environment. Isolation plays into the design of Mercury capsules, which wrap processing up in LXC-based virtual machines. We must also bear it in mind when considering the orchestration component, though it would be strange if pipelines were not conceptually isolated at this level. Care must be given to the interactions of isolation with optimisation of pipelines which share components. We wish to ensure that shared work is not duplicated, but carefully isolate the parts of processing unique to each pipeline.
\subsection{Pipeline Optimisation}
\npar It has been observed in many places that as the costs for sequencing drop at a faster rate than the costs for processing and storage, we will need to be significantly more efficient in how we process and store sequencing data. Even without this driver, however, we would like to take full advantage of the resources available to us for computation. We need to balance this with the semi-competing desire to be \textit{fair}, which is probably to say that there should be no strategy allowing any subgroup of users to guarantee control of an arbitrarily high proportion of the compute resources.
\npar Currently, there are a number of tools in place to try to aid in this process. VRPipe does some resource tracking to try to avoid duplicating previously run jobs, and makes some attempt to estimate the resource requirements of a job in order to avoid requesting more resources than are necessary. LSF then uses a `fairshare' system to allocate resources to users in a way which tries to maximise allocation whilst ensuring that everyone gets appropriate usage rights. Whilst these are good ideas, their implementation has much room for improvement. VRPipe does not handle inter-pipeline sharing and its resource estimation is rudimentary. Likewise, whilst the fairshare features in LSF are probably fair (whilst there are dominant strategies, they generally involve exploiting other parts of the system and so might be considered `cheating'), they do not encourage efficient resource usage: a common situation is for there to be a surfeit of CPUs available for low-memory jobs, but a disincentive to use them because this harms the global priority you might need to compete for more contested resources. People don't want to eat the boring biscuits in case they are placed lower in the queue for when the chocolate coated ones come around, and so we have a load of boring biscuits being passed around uneaten.
\npar We would like Mercury to take these ideas (and a few others) and develop them. This can be broken down into a few sub-themes:
\begin{enumerate}
\item Firstly, we would like to empirically determine the relative costs of storage and compute. By capturing rich data on how expensive a job is and how large are the data it generates, we can build a picture of a `reasonable' exchange rate between compute time (and other resources) and storage costs. For example, if we know that on average it costs 4 CPU-hours and 2 GB-hours of memory to produce 100GB of output data, then we might infer that something costing 10 CPU-hours and 5 GB-hours of memory but producing only 1GB of output is a very efficient usage of storage over compute. In terms of time/memory trade-offs, this is an ideal candidate for caching. Conversely, something costing a single CPU-hour and 512Mb hours of RAM but producing 50Gb of output is extremely cheap to produce compared to how much it costs to store, and we should almost never keep it around.
\item Separately, but relatedly, we would like to support demand-based pricing of the various resources available to the system (CPU, Memory, I/O etc) and a `fairshare' like system built around this pricing model. At the user-facing level, this would hopefully address some of the problems with the current system in terms of `cheap' resources being underutilised - where there is low demand on a particular resource, the price would drop until demand rose.

At the system level, this model would give us the additional information needed to engage in deliberate currency trading between storage and compute, providing another method to increase efficiency. For example, if we found ourselves with CPU resources in very high demand but a surfeit of disk space, we might choose to `buy' CPU resource by aggressively caching data, lowering the cost of compute at the expense of cost for disk space.\footnote{One can imagine a scenario where we might attempt to do similarly for resources such as CPU and memory by literally trading with other clusters - perhaps offering cheap pricing for CPU-intensive low-memory jobs in return for cheap access to lots of memory. One can imagine this, but we shall discuss it no further in this paper.}
\item Once we are in possession of the mechanisms for evaluating the costs of TMTOs and similar techniques, we would like an infrastructure capable of taking advantage of this as much as possible. To do this it is necessary to identify where multiple pipelines share components and aggressively de-duplicate processing. This should occur both with concurrent pipelines (where shared steps are 'merged') and with pipelines which repeat work previously done by another pipeline. As mentioned in section \ref{sec:provenance} on provenance, this must be done both on pipelines `as designed', where we de-duplicate steps which are conceptually the same, and `as executed', where we de-duplicate steps which are executed in the same way.

As an example of this last point, we consider three related pipelines which calculate summary statistics on a list of numbers. They can be described as follows:
\begin{enumerate}
\item Pipeline a runs on a file containing the numbers 0 to 99. It splits this file into parts $ \{F_n\}_{n \leq N} $ where $ F_n := [N*n, N*(n+1) \wedge 100) $. It then operates in parallel on each $F_n$, counting how many odd numbers are contained in $ F_n $. At the end, the results from each $ F_n $ are summed to give us the count of odd numbers in the original file.
\item Pipeline b is almost identical to pipeline a, except that a different number $ M \neq N $ is picked for the size of splits (We have not yet mentioned the mechanism by which this parameter might be `picked' - more on this in the next subsection).
\item Pipeline c behaves much as pipeline a, except that it operates on a file containing the numbers 0 to 49. The number $ N $ is chosen identically with pipeline a.
\end{enumerate}

In this case, pipelines a and b are both doing the same thing as designed. Abstracting over the detail for now, we would like to think of wrapping up the entire process of splitting, operating in parallel and summing into a single step, `count odd numbers', whose result should be fully determined by its input. Under this abstraction, pipelines a and b are identical, and so it should be valid to re-use the end results from a for pipeline b.

Pipelines a and c, on the other hand, are operating on different files, and the input to the `count odd numbers' step is different. However, when we split the file in c into its five parts, we see that those five parts have already been processed as part of pipeline a, and as such we can reuse those parts. Whilst the pipelines are different in design, they share steps as executed, and these steps can also be de-duplicated.
\item In the previous point, we alluded to the idea of `picking' $N$, the chunk size to split the file into before processing chunks in parallel. Parameters such as $N$ may exist in many pipelines - parameters which do not (or should not) affect the outcome of the pipeline but which may have a massive effect on the runtime behaviour and performance characteristics. Taking $N$ as an example, for very high values of $N$ we have very large lists of numbers being processed in serial on a single machine, which could be very slow if we were doing more than counting odd numbers. Conversely, if we pick $N$ to be very small (1, say), we split the file into a very large number of jobs, each of whose overheads are likely to dominate the actual cost of processing the file. Somewhere between these is an optimal value which balances the overheads of multiple processing with the parallelism offered.

Furthermore, since we do not just care about the fastest execution, but about the total \textit{cost} of the job, the best parameter values will depend not only on the job but on the demand and availability of resources. For example, if there are currently a large number of single-CPU low-memory slots available in our earlier example, it may be advantageous to set $N$ low in order to fit into the low-memory slots, whilst if there are large machines around we may wish to simply use a single machine to blow through the whole list.

For cases such as splitting files and operating in parallel, but potentially other parameters, we would like the system to be able to choose a value for the parameter to take advantage of its knowledge of current resource prices and empirical data on the execution parameters of that job. This shifting of responsibility from the user to the system should allow for a more efficient execution, but also removes some of the control from the user in terms of their specific goals. For example, they might not wish to minimise cost but just get the fastest execution regardless of price. To cover this situation, we suggest that the system might implement different strategies for choosing parameter values. The default strategy might be to minimise cost, but another could attempt to minimise runtime, or minimise runtime subject to a cap on cost.
\end{enumerate}
\npar In terms of requirements resulting from these goals, there are a number. As already mentioned in sections \ref{sec:provenance} and \ref{sec:repeatability}, we need to track data about the resources used in pipeline execution and storage. We need the flexibility to perform trade-offs between storage and compute, which means the system needs to `own' its own storage, able to perform deletions or compaction without this being noticed by users. We need to track provenance information in order to decide whether data should be reused from a previous run. We also need a way of indicating that a given step is \textit{not} recreatable - for example, because it talks to an external resource which we do not govern.
\npar The need to de-duplicate at multiple levels, and to allow certain parameters to be picked by the runtime, requires that we have some conception (or at least record) of pipelines at multiple levels of abstraction, and to track the results as seen from each of those levels.
\subsection{Security}
\npar After the \textit{tour de force} of optimisation, there is comparatively little to say about security, which is mostly interesting for how it interacts with some of the other goals (principally optimisation). We break down security into the classic three security attributes of confidentiality, integrity and availability:
\begin{itemize}
\item \textbf{Confidentiality} - The Mercury system will occasionally be required to process vaguely personally-attributable data which could be considered sensitive. It needs to ensure that data inside its system remains accessible only to those with the correct permissions, and that any data it imports on behalf of users is accessed with their permissions only. However, this should be done whilst trying to avoid multiple independent copies of sensitive data being worked on separately within the system. 
\end{itemize}
\end{document}
