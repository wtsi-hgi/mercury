\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[colorlinks]{hyperref}
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{listings}
\usepackage[style=long,toc,xindy]{glossaries}
\makeglossaries
\setlength{\parindent}{15pt}
\author{Nicholas Clarke}
\title{Mercury specification}
\newcounter{paracounter}
\newcommand{\npar}{\par\noindent\refstepcounter{paracounter}\theparacounter.\space}
\include{glossary}
\begin{document}
\maketitle
\begin{abstract}
We present a specification for Mercury, a hypothetical pipeline system designed for use in human genetics workflows. Drawn from previous experience managing and running genetics pipelines, we identify a number of specific capabilities that would be desirable for the purpose of provenance tracking, repeatability and optimality of experiment workflows.
\end{abstract}
\tableofcontents
\section{Introduction}
\npar Within human genetics, there is a need to be able to design and execute specific \glspl{workflow} (\glspl{pipeline}) for genome analysis on a reliable, repeatable and efficient basis. At present, HGI runs a variety of pipelines for human genetics projects. These are run against a Linux cluster using Lustre-backed storage by the following two tools: 
\begin{itemize}
\item \href{https://github.com/VertebrateResequencing/vr-pipe}{VRPipe} runs as the \textit{orchestration} framework for current pipelines. It takes a description of the workflow (as a Perl script) and is responsible for tracking dependencies between jobs, submitting them to the executor and monitoring their progress.
\item \href{http://www-03.ibm.com/systems/technicalcomputing/platformcomputing/products/lsf/}{LSF} runs as the executor, responsible for taking an individual job and executing it on some compute resource. LSF additionally has the ability to manage job dependencies, but these features are not currently in use at the Sanger.
\end{itemize}
\npar Whilst these tools work suitably well for the general usage pattern at the Sanger, they show a number of limitations which it would be desirable to work around. In particular, HGI is interested in the areas of \textbf{data provenance}, \textbf{experiment repeatability}, \textbf{experiment isolation}, \textbf{pipeline optimisation} and \textbf{pipeline security}, hereafter referred to as PRIOS.
\npar Human Genetics has recently taken delivery of a new cluster (hgs4) intended primarily for pipeline usage. This presents us with the opportunity to re-evaluate our current toolset and practices and think about what we would really like in a pipeline execution system. This document presents a write-up of a series of meetings held on the topic, and is intended to serve as a specification for a potential future system embodying these desires.
\npar This document should ideally present the requirements for such a future system, and serve equally well as a guideline for the design of a new system or as a standard to evaluate existing systems by. Inevitably, however, we shall fall into the trap of starting to design a new system rather than just specify it. Where possible, sections that veer into design or even implementation will be flagged as such; it is quite likely, however, that some will escape. The reader is asked to bear this in mind when applying this document.
\section{Goals}
\npar In the introduction, we introduced some high-level goals in which HGI has a particular interest. These are not requirements in their own right, being too high-level, but should give rise to requirements in the same way as other top-level goals such as 'the system must be usable'. In this section, we'll take a look at these goals, elaborating on and discussing them in order to provide context and motivation for the requirements introduced in the following sections.
\subsection{PRIOS Goals}
\subsubsection{Data Provenance}
\label{sec:provenance}
\npar Data provenance is fundamentally about being able to ask the question ``Where did this data come from?'' For a given set of data, we want to be able to identify which pipeline(s) it was created from, what steps were involved, where the initial data came from, how the jobs were distributed amongst machines and what was running on the machines at the time? As far as possible, we would like to be able to identify precisely what happened to cause the data to be generated as they were.
\npar There are various justifications for this. Partially, it is about repeatability (as in the next section) - being able to identify how something came about makes it much easier to repeat it. Partially it is about data integrity - being certain that results were generated using the precise input data which you thought they were. Optimisation also calls for provenance data - once we understand how precisely data were generated, we know how expensive they are to recreate and how important it is to store them. And, in part, we want to be able to document the process for users' own purposes, so they can check what happened as part of an analysis run by HGI.
\npar Implementing provenance tracking leads to a number of requirements. We need to keep track of pipelines both as designed (in general, without specific reference to data) and as executed (on which hosts, at what time). We need to ensure that external dependencies are explicitly captured (which in turn leads to the isolation goal), and isolated both in the planning stage (separating external I/O from internal - e.g. passing data between steps in a pipeline) and in the execution stage, preventing steps from unknowingly accessing external resources.
\subsubsection{Experiment Repeatability}
\label{sec:repeatability}
\npar Following on from provenance, we have the goal of making our pipelines repeatable. This goes a step further than provenance; rather than simply identifying where data comes from, we would like to be able to recreate it, in spite of potential changes to the underlying infrastructure. 
\npar This gives rise to the ideas of encapsulation already implemented in Mercury capsules, wrapping up an environment suitable to run a step of the pipeline. Whilst capsules provide repeatability of individual steps, we are also forced to consider repeatability of the entire pipeline, which requires us to suitably abstract away from specific hardware or infrastructure constraints. This is one advantage to moving away from the current LSF-based approach, for example.
\npar Also covered under repeatability is the requirement to rerun an experiment whilst changing a fixed number of factors, and tracking effects. For example, we might want to rerun an old pipeline using a new version of a particular piece of software to see whether it meaningfully alters the output. This same idea applies to rerunning a pipeline on new data.
\subsubsection{Experiment Isolation}
\npar Isolation is, in many ways, an enabling goal. Isolation of experiment processing allows us to be certain of the provenance of generated results, and enables us to move the isolated experiment to another system to repeat it. By isolating each pipeline (and each step of the pipeline), we enable meaningful statistics to be gathered about resource usage and running profile to allow for optimisation, and prevent other systems from interfering with the processing in order to provide security.
\npar Isolation principally sets requirements on the execution of individual jobs, where we need to isolate the actual processing from other jobs on the same host, as well as from the wider environment. Isolation plays into the design of Mercury capsules, which wrap processing up in LXC-based virtual machines. We must also bear it in mind when considering the orchestration component, though it would be strange if pipelines were not conceptually isolated at this level. Care must be given to the interactions of isolation with optimisation of pipelines which share components. We wish to ensure that shared work is not duplicated, but carefully isolate the parts of processing unique to each pipeline.
\subsubsection{Pipeline Optimisation}
\label{sec:optimisation}
\npar It has been observed in many places that as the costs for sequencing drop at a faster rate than the costs for processing and storage, we will need to be significantly more efficient in how we process and store sequencing data. Even without this driver, however, we would like to take full advantage of the resources available to us for computation. We need to balance this with the semi-competing desire to be \textit{fair}, which is probably to say that there should be no strategy allowing any subgroup of users to guarantee control of an arbitrarily high proportion of the compute resources.
\npar Currently, there are a number of tools in place to try to aid in this process. VRPipe does some resource tracking to try to avoid duplicating previously run jobs, and makes some attempt to estimate the resource requirements of a job in order to avoid requesting more resources than are necessary. LSF then uses a `fairshare' system to allocate resources to users in a way which tries to maximise allocation whilst ensuring that everyone gets appropriate usage rights. Whilst these are good ideas, their implementation has much room for improvement. VRPipe does not handle inter-pipeline sharing and its resource estimation is rudimentary. Likewise, whilst the fairshare features in LSF are probably fair (whilst there are dominant strategies, they generally involve exploiting other parts of the system and so might be considered `cheating'), they do not encourage efficient resource usage: a common situation is for there to be a surfeit of CPUs available for low-memory jobs, but a disincentive to use them because this harms the global priority you might need to compete for more contested resources. People don't want to eat the boring biscuits in case they are placed lower in the queue for when the chocolate coated ones come around, and so we have a load of boring biscuits being passed around uneaten.
\npar We would like Mercury to take these ideas (and a few others) and develop them. This can be broken down into a few sub-themes:
\begin{enumerate}
\item Firstly, we would like to empirically determine the relative costs of storage and compute. By capturing rich data on how expensive a job is and how large are the data it generates, we can build a picture of a `reasonable' exchange rate between compute time (and other resources) and storage costs. For example, if we know that on average it costs 4 CPU-hours and 2 GB-hours of memory to produce 100GB of output data, then we might infer that something costing 10 CPU-hours and 5 GB-hours of memory but producing only 1GB of output is a very efficient usage of storage over compute. In terms of time/memory trade-offs, this is an ideal candidate for caching. Conversely, something costing a single CPU-hour and 512Mb hours of RAM but producing 50Gb of output is extremely cheap to produce compared to how much it costs to store, and we should almost never keep it around.
\item Separately, but relatedly, we would like to support demand-based pricing of the various resources available to the system (CPU, Memory, I/O etc) and a `fairshare' like system built around this pricing model. At the user-facing level, this would hopefully address some of the problems with the current system in terms of `cheap' resources being underutilised - where there is low demand on a particular resource, the price would drop until demand rose.

At the system level, this model would give us the additional information needed to engage in deliberate currency trading between storage and compute, providing another method to increase efficiency. For example, if we found ourselves with CPU resources in very high demand but a surfeit of disk space, we might choose to `buy' CPU resource by aggressively caching data, lowering the cost of compute at the expense of cost for disk space.\footnote{One can imagine a scenario where we might attempt to do similarly for resources such as CPU and memory by literally trading with other clusters - perhaps offering cheap pricing for CPU-intensive low-memory jobs in return for cheap access to lots of memory. One can imagine this, but we shall discuss it no further in this paper.}
\item Once we are in possession of the mechanisms for evaluating the costs of TMTOs and similar techniques, we would like an infrastructure capable of taking advantage of this as much as possible. To do this it is necessary to identify where multiple pipelines share components and aggressively de-duplicate processing. This should occur both with concurrent pipelines (where shared steps are 'merged') and with pipelines which repeat work previously done by another pipeline. As mentioned in section \ref{sec:provenance} on provenance, this must be done both on pipelines `as designed', where we de-duplicate steps which are conceptually the same, and `as executed', where we de-duplicate steps which are executed in the same way.

As an example of this last point, we consider three related pipelines which calculate summary statistics on a list of numbers. They can be described as follows:
\begin{enumerate}
\item Pipeline a runs on a file containing the numbers 0 to 99. It splits this file into parts $ \{F_n\}_{n \leq N} $ where $ F_n := [N*n, N*(n+1) \wedge 100) $. It then operates in parallel on each $F_n$, counting how many odd numbers are contained in $ F_n $. At the end, the results from each $ F_n $ are summed to give us the count of odd numbers in the original file.
\item Pipeline b is almost identical to pipeline a, except that a different number $ M \neq N $ is picked for the size of splits (We have not yet mentioned the mechanism by which this parameter might be `picked' - more on this in the next subsection).
\item Pipeline c behaves much as pipeline a, except that it operates on a file containing the numbers 0 to 49. The number $ N $ is chosen identically with pipeline a.
\end{enumerate}

In this case, pipelines a and b are both doing the same thing as designed. Abstracting over the detail for now, we would like to think of wrapping up the entire process of splitting, operating in parallel and summing into a single step, `count odd numbers', whose result should be fully determined by its input. Under this abstraction, pipelines a and b are identical, and so it should be valid to re-use the end results from a for pipeline b.

Pipelines a and c, on the other hand, are operating on different files, and the input to the `count odd numbers' step is different. However, when we split the file in c into its five parts, we see that those five parts have already been processed as part of pipeline a, and as such we can reuse those parts. Whilst the pipelines are different in design, they share steps as executed, and these steps can also be de-duplicated.
\item In the previous point, we alluded to the idea of `picking' $N$, the chunk size to split the file into before processing chunks in parallel. Parameters such as $N$ may exist in many pipelines - parameters which do not (or should not) affect the outcome of the pipeline but which may have a massive effect on the runtime behaviour and performance characteristics. Taking $N$ as an example, for very high values of $N$ we have very large lists of numbers being processed in serial on a single machine, which could be very slow if we were doing more than counting odd numbers. Conversely, if we pick $N$ to be very small (1, say), we split the file into a very large number of jobs, each of whose overheads are likely to dominate the actual cost of processing the file. Somewhere between these is an optimal value which balances the overheads of multiple processing with the parallelism offered.

Furthermore, since we do not just care about the fastest execution, but about the total \textit{cost} of the job, the best parameter values will depend not only on the job but on the demand and availability of resources. For example, if there are currently a large number of single-CPU low-memory slots available in our earlier example, it may be advantageous to set $N$ low in order to fit into the low-memory slots, whilst if there are large machines around we may wish to simply use a single machine to blow through the whole list.

For cases such as splitting files and operating in parallel, but potentially other parameters, we would like the system to be able to choose a value for the parameter to take advantage of its knowledge of current resource prices and empirical data on the execution parameters of that job. This shifting of responsibility from the user to the system should allow for a more efficient execution, but also removes some of the control from the user in terms of their specific goals. For example, they might not wish to minimise cost but just get the fastest execution regardless of price. To cover this situation, we suggest that the system might implement different strategies for choosing parameter values. The default strategy might be to minimise cost, but another could attempt to minimise runtime, or minimise runtime subject to a cap on cost.
\end{enumerate}
\npar In terms of requirements resulting from these goals, there are a number. As already mentioned in sections \ref{sec:provenance} and \ref{sec:repeatability}, we need to track data about the resources used in pipeline execution and storage. We need the flexibility to perform trade-offs between storage and compute, which means the system needs to `own' its own storage, able to perform deletions or compaction without this being noticed by users. We need to track provenance information in order to decide whether data should be reused from a previous run. We also need a way of indicating that a given step is \textit{not} recreatable - for example, because it talks to an external resource which we do not govern.
\npar The need to de-duplicate at multiple levels, and to allow certain parameters to be picked by the runtime, requires that we have some conception (or at least record) of pipelines at multiple levels of abstraction, and to track the results as seen from each of those levels.
\subsubsection{Security}
\npar After the \textit{tour de force} of optimisation, there is comparatively little to say about security, which is mostly interesting for how it interacts with some of the other goals (principally optimisation). We break down security into the classic three security attributes of confidentiality, integrity and availability:
\begin{itemize}
\item \textbf{Confidentiality} - The Mercury system will occasionally be required to process vaguely personally-attributable data which could be considered sensitive. It needs to ensure that data inside its system remains accessible only to those with the correct permissions, and that any data it imports on behalf of users is accessed with their permissions only. However, this should be done whilst trying to avoid multiple independent copies of sensitive data being worked on separately within the system. Where multiple people are working on the same sensitive data, the system needs to be intelligent enough to realise this and update the permissions accordingly.
\item \textbf{Integrity} - Data integrity covers the ability of the system to ensure the correctness of generated data even in the presence of errors, along with the ability to provide sufficient evidence for that correctness. The latter, for a previously generated dataset, is covered mostly by the trail of provenance data which we collect, dealt with already in section \ref{sec:provenance}. Ensuring the correctness of data in the presence of errors will require both a resilient system capable of detecting and sensibly handling errors (dealt with in the later section on \ref{sec:resilience}), and a system of checks and balances which guard against errors outside of the control of the system - for example, capable of detecting corruption in data as it moves between steps.
\item \textbf{Availability} - This comes in two flavours - availability of generated results, and availability of the pipeline system. The latter we will cover later in the more appropriate later section \ref{sec:resilience} on resilience. The former involves the semantics for data which leaves the system's direct control. In general, we may not wish to maintain separate copies of the data inside Mercury's control and outside of it. However, data that has left the system is no longer under the total control of Mercury - it cannot decide to delete it, for example. Some conception would be desirable for referencing data which has left the system in such a way that it can still be drawn back in for pipeline use.
\end{itemize}
\subsection{Other goals}
\npar The PRIOS goals represent a specific area in which HGI is particularly interested in extending our pipeline systems. However, there are also a number of other goals which are intrinsic to the proposed system being useful and suitable as a general replacement for the current Mercury pipeline system. We address these in the following section.
\subsubsection{Resilience}
\npar Running over a large-scale cluster, errors with individual low probability become a regular occurrence, and we therefore require our system to deal with them accordingly. Further, leaving aside even the error case, the Mercury system will rely on a number of tools outside its control. In the initial conception, Mercury at the Sanger will be running on a number of private compute nodes as well as some shared with other uses, using two or three distinct Lustre storage units for scratch storage. Any of these units may be taken down for reasons outside of Mercury's (or HGI's) control, and Mercury must be able to cope with these outages.
\npar Resilience starts with the ability to self-monitor; Mercury needs to be able to detect failures and handle them gracefully. These failures may occur in many different places: in an individual job, on a host, at the network level, in the job planner, with a specific resource (e.g. a particular Lustre unit), with external I/O, and so on. Each of these error modes may need to be handled differently. The system needs to be able to identify various classes of error, handle immediate consequences, perhaps investigate further, maintain an internal awareness of what is working and what is not, and report back to users, HGI or Systems.
\npar The first responsibility upon detecting an error is to ensure the integrity of associated data. Where errors have been detected on a node, or with a particular resource, this may taint recent results which have been generated on that node or using that resource. The pipeline may need to decide to backtrack and rerun certain steps whose results are now suspect. A failing node will need to be closed to new jobs until the system (or a user) can verify it is working correctly. Where there are more transient errors, such as reading a file from disk and getting an invalid checksum, it may only be necessary to restart the step in question, though if such an error persists then the pipeline may need to backtrack.
\npar After guaranteeing the integrity of working data following an error, the system should also attempt to continue running to the best of its ability with whatever capability it has remaining. If a pipeline depends on data which is no longer available, it should attempt to recreate that data in another location. Jobs should be restarted on working hosts. Of course, failures in the system may well result in large price increases as a lower number of resources are available to service an increased demand!
\label{sec:resilience}
\subsubsection{Flexibility}
\label{sec:flexibility}
\npar The current pipelines used by HGI are somewhat specific to Sanger infrastructure, tied to LSF and Lustre and our current use of them. Whilst this is in no way a problem at present, we can envision wanting to replace these at some time in the future, or to hook other resources into the system and allow them to be used. We may also wish to run Mercury at places other than the Sanger which have quite different infrastructure.
\npar We would therefore like to be generally flexible as to the required infrastructure supporting Mercury. Without going into too much detail, here are some ideas of things which we do and do not expect to rely on.
\begin{itemize}
\item Things we expect to rely on:
\begin{enumerate}
\item Linux - this is an implicit requirement of many of the tools we use, and there's almost no call for running on anything else.
\end{enumerate}
\item Things we may come to rely on:
\begin{enumerate}
\item LXC - this is currently our expected method for containerisation of jobs, and we don't think this will change. However, it's possible that the \glspl{job executor} could be made sovereign of the container mechanism used to actually run its jobs, subject to a sufficiently rich interface between the two. However, it's not clear that it's worth the cost of doing this.
\item Mesos - or at least a similar meta-scheduler. This is not a necessary dependency, and the interface is very simple, which is why it seems fair to consider it. Mercury could operate directly against compute resources which it owns, but this would require it to implement much of Mesos's functionality whilst losing the benefit of running other schedulers under it.
\end{enumerate}
\item Things we would like not to rely on:
\begin{enumerate}
\item Storage mechanisms - current pipelines are reliant on traditional Posix-based file systems for their working storage. However, there are often better choices to be made for certain types of access pattern. For example, one way of abusing Lustre is to write millions of small files, which causes congestion on the metadata server. A key-value style store, however, is perfect for storing many small documents, so may well be a better choice for this access pattern. Where small amounts of data are being passed around between jobs, we may not want to go to the expense of writing to disk at all, and use something like a memcached server to persist data between jobs.

Mercury will ideally be able to support multiple different backing storage mechanisms, and be able to choose between them according to the particular situation.
\item Host software - by taking advantage of containerisation we aim to reduce the requirements on the individual host to being able to run a job executor and a container. This should eliminate problems with the software configuration on a host.
\end{enumerate}
\end{itemize}
\subsubsection{Usability and accessibility}
\npar TODO - this section has not yet been discussed!
\subsubsection{Scalability}
\npar As our demands for computation increase, it may be increasingly implausible to carry out all computation in house using pre-assigned resources. Whilst we assume that the hgs4 cluster may run Mercury constantly, we may also wish to use other resources such as the general clusters to expand Mercury on a temporary basis, either to cope with periods of increased demand, or to deal with particular jobs possessing unusual requirements.
\npar Mercury needs to be built to cope with the idea of its fundamental resources - whether in the form of compute nodes or storage resources - being transitory, subject to expansion or removal even during pipeline execution. It should be easy to add a new resource for use by Mercury and have existing Pipelines seamlessly modified to take advantage of it. Likewise when a resource is removed from the system.
\npar Using \href{http://mesos.apache.org/}{Mesos} as in our preferred deployment option would provide the above capability for compute nodes. However, we still need to consider how to do the same for storage resources (or any other resources not considered here), and if we do not use Mesos (or a similar solution) then we must support this for compute resources as well.
\subsection{Miscellany}
\npar From our experience in dealing with existing requirements have come about a number of use cases which do not fit neatly into the high-level goals set out above, but which nonetheless represent concerns we should take into consideration when designing our system. We expect that these cases may be considered as extensions to the system rather than critical parts of the core system. However, the ability to admit such extensions must be seen as a goal for the system itself.
\subsubsection{Checkpointing}
\npar The information available to the scheduler when it comes to making decisions about which jobs to schedule is sadly imperfect. As far as possible, it should take advantage of the knowledge available to it - the size of the input, empirical data about how long a step usually takes to run, etc. - to make reasonable assumptions about the resource profile of a job, but inevitably these assumptions will be incorrect.
\npar The scheduler should be perfectly happy to make decisions based upon this incorrect knowledge. However, it should also be able to change its mind at a later point when further information becomes available. One possibility for this is to simply kill the job and reschedule it later, but this may be expensive if it has already carried out significant amounts of work. The alternative is to take advantage of a checkpointing system which will try to save the state of a job for subsequent resumption later.
\npar Checkpointing may also be used for other reasons. For example, if we know a host is going down (see section \ref{sec:outagePlanner} below) then we might want to suspend all jobs running on it and move them to another host. We can also do this if we believe a job is trying to expand beyond the resources available to it.
\npar Checkpointing itself is a feature offered by external tools. Currently we use Berkeley Linux Checkpoint/Restart, although there are potentially newer implementations within the Linux kernel itself. Mercury should not express a dependency on any one of these tools, but needs to understand the concept of a suspended job, and the mechanisms for suspending and then restarting a job as part of normal resource management.
\subsubsection{Generalised Assertions}
\npar One of the fundamental ideas we wish to take advantage of is that for certain steps (\textit{pure} or \textit{referentially transparent} steps), we may replace an invocation of a step $ f $ on arguments $ (a_i) $ with the result of a prior invocation of that same $ f $ on those same arguments. Even where we do not have those results cached, we may use this identity to reason about subsequent behaviour of the pipeline - for example, we may decide that we don't need to run it at all if we believe it will generate precisely the same results as a previously executed pipeline.
\npar This observation lies at the heart of much of the optimisation strategy detailed above in section \ref{sec:optimisation}. However, it is in many ways a blunt instrument, with only two possibilities. Sometimes we would like to be more general. For example, we might wish to state that a particular output of a step depends on one of its inputs, but not the other. Alternatively, we might get a new version of a piece of software which we wish to use for performance reasons, but where we believe that the results should not differ from the previous version. In this case, we might assert that any results generated by the previous version can be used for pipelines running the new version, and vice versa.
\npar We call such things generalised assertions. Generalised assertions are principally of use to the pipeline planning system - they determine where it should look to determine whether or not a step of a pipeline needs to run.
\subsubsection{Quickcheck-style self testing}
\npar In the previous section, we discussed the purity and how we could use this for pipeline optimisations. However, because we have actually worked with some of the software these pipelines will be running, we are a little sceptical of claims made by particular programs to being pure. As such, we would like Mercury to follow a `trust and verify' scheme. In general, we will assume that they do what they say, and will always return the same output for the same input. Sometimes, however, particularly when resources are very cheap owing to low utilisation, the system may decide to rerun supposedly pure components in order to verify that they are doing what they claim to do.
\npar We may wish to do the same thing for any of the generalised assertions mentioned in the previous section. On occasion, we might want to go the other way - rather than trust and verify, we would distrust until we had enough confidence in the result, and only then assert such behaviour. This seems particularly likely in cases where a particular version of software is updated - we may not wish to trust anything without checking it first.
\npar Mercury should ideally allow for support of both styles of property testing in a nice way, using previously run experiments as the source of data to test the system with.
\subsubsection{Outage planner}
\label{sec:outagePlanner}
\npar In contrast to errors, we often know well in advance when outages for critical resources are going to occur. Given this, we should be able to take advantage of this foreknowledge in order to better make use of resources in and around the outage.
\npar For example, on receiving knowledge that a storage system is going down, we might identify resources which are going to be needed by queued pipelines and begin moving them to another resource in advance of the outage. Meanwhile, very short or cheap jobs could be moved to using the soon to be outed resource in the knowledge that they will either finish before the outage or be cheap enough to replicate if taken down by it.
\npar Ideally, one would be able to tell Mercury about the outage schedule of particular resources and have this information used when deciding how to assign resources to particular pipelines or jobs.
\subsubsection{Streaming}
\npar In order to execute a pipeline, Mercury will need to perform a partial linearisation of the dependency graph - in other words, forcing jobs that serve as dependencies for others to be performed first. This will not form a total order, since some jobs may be independent of each other and may run simultaneously. In general, within each ordered chain, jobs will be executed sequentially, with one job running, outputting, and exiting before the next is allowed to start. This is a sensible strategy to assume for programs which, say, write their output to a file which another program will attempt to perform random accesses into.
\npar However, Unix has long supported another method of sequencing steps in a computation using its streaming pipe operator. In this model, data is streamed directly from one job to the next without requiring writing to disk in the meantime. This works very well for jobs which simply read a file sequentially.
\npar One way for Mercury to support this would be to require each chain of streaming components to be packaged as a single step, and rely on the underlying host to handle the streaming. This is below optimal for many reasons. Firstly, it increases the size and complexity of the \glspl{primitive step}, which decreases compositionality and increases duplication. Secondly, it it excludes all of Mercury's checksums, caches and other optimisations taking place between components (see figure \ref{fig:streaming:bigPrimitives}). Finally, we limit streaming composition of jobs to a single host, which raises the bar for hosts we can use and again decreases Mercury's ability to optimise for best usage of its resources.
\begin{figure}
\lstset{language=Bash,
		xleftmargin=.2\textwidth,
		xrightmargin=.2\textwidth
		}
\begin{lstlisting}
seq 1 1000000 | filterPrimes
seq 1 1000000 | filterPrimes | head -n 5
\end{lstlisting}
\caption{If we run the two commands above as separate primitive steps, we cannot take advantage of any caching or optimisation between them, and so we must compute the expensive `filterPrime' each time, despite the fact that we could re-use it. }
\label{fig:streaming:bigPrimitives}
\end{figure}
\npar As such, we would like for Mercury to support this at a higher level, allowing jobs to be connected together with `streaming' arrows rather than conventional ones. Clearly, this would only be possible for certain jobs which advertise to work with streaming data, although since streaming can be seen as more general than a sequential approach, it should be possible to connect streaming jobs to conventional ones through simply streaming to/from disk, although with none of the benefits.
\npar Connecting two streamable blocks with a streaming arrow would effect the following changes in behaviour:
\begin{enumerate}
\item The second job could be scheduled to start as soon as the first job starts, rather than as soon as the first job finishes. In general, we would like the jobs to start as concurrently as possible, in order as to not have the first job blocking on I/O whilst waiting for the second.
\item Rather than output and input being connected to one of the standard resources, they would be connected to a `special' resource which would not store data but stream it between them. Note, however, that this resource may choose to do other things, such as checksumming the data or copying it to another resource for storage.
\end{enumerate}
\subsubsection{MPI}
\npar Mercury as we have described it is designed mainly for running, in parallel, a large number of jobs which have no inter-dependencies. Where dependencies do exist, linearisation is performed to ensure that one job must complete before the next can start. It might be nice to allow for the running of jobs which are inherently parallel, requiring deployment onto multiple hosts but managing the process of inter-host communication themselves. MPI is the canonical example of how such a job might work.
\npar There is a reasonably strong argument that Mercury would be the wrong framework to run such jobs in, however. Firstly, Mercury is intended (see section \ref{sec:flexibility}) to be flexible as to its underlying hardware and network model. Making direct network communications to other nodes available to running jobs would break that abstraction somewhat, and could be problematic when some parts of a job are deployed onto different network segments, or even sent off to Amazon or somewhere for processing.
\npar This objection could be worked around with judicious use of a resource constraints approach, where a job could be specified to require a certain network or similar. However, the second major objection to this behaviour is that it requires out-of-band communication between nodes, making it more difficult to ensure things such as data provenance and repeatability.
\npar This is not to say that we do not see a place for such jobs; indeed, we believe that their importance is only going to grow. However, it may be that they are better handled by higher level frameworks such as Hadoop which gain more powerful semantics for reasoning about parallel jobs through the sacrifice of flexibility as to how a job operates. Our preferred scenario for the deployment of Mercury has it running atop of \href{http://mesos.apache.org/}{Mesos}, a meta-scheduler allowing resources to be shared between multiple scheduling frameworks. This would allow Mercury to run alongside Hadoop, servicing different type of jobs.
\npar All of that having been said, it may be that there is still a desire to include MPI-type jobs into the purview of Mercury. This problem is unlikely to be insurmountable, but would require that we consider safe ways of constraining network communication, of passing \glspl{metadata} (e.g. host addresses) into jobs, and of generalised resource constraints which would allow, for example, a \gls{step} to specify that all jobs resulting from it must be deployed onto the same network segment. 
\printglossaries
\end{document}
